{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                # Sepparating each word from its embedding\n",
    "                word, vec = line.split(' ', 1)\n",
    "                # We add each word and its embedding to the word2vec attribute\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort\n",
    "        key_list = []\n",
    "        key_score = []\n",
    "        # Creating an array with the cosine score for each word\n",
    "        for key, value in self.word2vec.items():\n",
    "            key_list.append(key)\n",
    "            key_score.append( self.score(w, key) )\n",
    "        # Sorting the words\n",
    "        key_list = np.array(key_list)\n",
    "        key_score = np.array(key_score)\n",
    "        sorted_score = np.argsort( key_score )\n",
    "        # \n",
    "        most_similar_list = key_list[sorted_score[(-K-1):-1]]\n",
    "        return most_similar_list\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # Calculating the cosine similarity: np.dot  -  np.linalg.norm\n",
    "        cosine_similarity = ( np.dot(self.word2vec[w1], self.word2vec[w2]) ) / ( np.linalg.norm( self.word2vec[w1] ) * np.linalg.norm( self.word2vec[w2] ) )\n",
    "        return cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052403\n",
      "paris france 0.7775108541288561\n",
      "germany berlin 0.7420295235998392\n",
      "['kitties' 'feline' 'kitten' 'kitty' 'cats']\n",
      "['canine' 'doggie' 'Dog' 'puppy' 'dogs']\n",
      "['canines' 'doggies' 'Dogs' 'pooches' 'dog']\n",
      "['tokyo' 'berlin' 'london' 'Paris' 'france']\n",
      "['poland' 'berlin' 'german' 'europe' 'austria']\n"
     ]
    }
   ],
   "source": [
    "# We set nmax to 100,000 to be able to reach paris\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "        #\n",
    "        shape = (300,)\n",
    "        \n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                word_list = sent.split(' ')\n",
    "                mean = np.zeros(shape)\n",
    "                int_num = 0\n",
    "                for i in word_list:\n",
    "                    if i in w2v.word2vec.keys():\n",
    "                        mean += w2v.word2vec[i]\n",
    "                        int_num += 1\n",
    "                mean = mean / int_num\n",
    "                sentemb.append(mean)\n",
    "                #assert False, 'TODO: fill in the blank'\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                word_list = sent.split(' ')\n",
    "                mean = np.zeros(shape)\n",
    "                int_num = 0\n",
    "                for i in word_list:\n",
    "                    if i in w2v.word2vec.keys():\n",
    "                        mean += w2v.word2vec[i] * idf[i]\n",
    "                        int_num += 1\n",
    "                mean = mean / int_num\n",
    "                sentemb.append(mean)\n",
    "                #assert False, 'TODO: fill in the blank'\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        key_list = sentences\n",
    "        key_score = []\n",
    "         # Creating an array with the cosine score for each word\n",
    "        for sent in sentences:\n",
    "            key_score.append( self.score(s, sent, idf) )\n",
    "        # Sorting the sentences\n",
    "        key_list = np.array(key_list)\n",
    "        key_score = np.array(key_score)\n",
    "        sorted_score = np.argsort( key_score )\n",
    "        # \n",
    "        most_similar_list = key_list[sorted_score[(-K-1):-1]]\n",
    "        print(most_similar_list)\n",
    "        return most_similar_list\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        cosine_similarity = ( np.dot( self.encode([s1], idf)[0], self.encode([s2], idf)[0]) ) / ( np.linalg.norm( self.encode([s1], idf)[0] ) * np.linalg.norm( self.encode([s2], idf)[0] ) )\n",
    "        return cosine_similarity\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        length = len(sentences)\n",
    "        for sent in sentences:\n",
    "            word_list = sent.split(' ')\n",
    "            for w in set(word_list):\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "        for w in idf.keys():\n",
    "            idf[w] = max(1, np.log10(length / (idf[w])))\n",
    "        return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "['a girl in black hat holding an african american baby .'\n",
      " 'an african american man is sitting .'\n",
      " 'an afican american woman standing behind two small african american children .'\n",
      " 'a little african american boy and girl looking up .'\n",
      " 'an african american man smiling .']\n",
      "0.5726258859719607\n",
      "['a girl in black hat holding an african american baby .'\n",
      " 'an afican american woman standing behind two small african american children .'\n",
      " 'a little african american boy and girl looking up .'\n",
      " 'an african american man is sitting .'\n",
      " 'an african american man smiling .']\n",
      "0.4751450875368781\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "file = open( os.path.join(PATH_TO_DATA, 'sentences.txt'), 'r') \n",
    "sentences = file.readlines() \n",
    "sentences = [ x[:-2] for x in sentences]\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "print(s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13]))\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = s2v.build_idf(sentences)\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "print(s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\** = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "w2v_en=Word2vec('data/wiki.en.vec',nmax=50000)\n",
    "w2v_fr=Word2vec('data/wiki.fr.vec',nmax=50000)\n",
    "wiki_en=w2v_en.word2vec\n",
    "wiki_fr=w2v_fr.word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18970, 300)\n",
      "(18970, 300)\n"
     ]
    }
   ],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "# First, let us create a list of the common words\n",
    "intersect = []\n",
    "for item in wiki_en.keys( ):\n",
    "    if item in wiki_fr.keys():\n",
    "        intersect.append(item)\n",
    "\n",
    "# Creation of X and Y\n",
    "n = len(intersect)\n",
    "m = wiki_fr['le'].shape[0]\n",
    "\n",
    "X = np.zeros((n,m))\n",
    "Y = np.zeros((n,m))\n",
    "\n",
    "for i, item in enumerate(intersect):\n",
    "    X[i:]= wiki_en[item]\n",
    "    Y[i:]= wiki_fr[item]\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300)\n"
     ]
    }
   ],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "# TYPE CODE HERE\n",
    "# We will use the transposed of X and Y for computational reasons\n",
    "X_T = np.transpose(X)\n",
    "Y_T = np.transpose(Y)\n",
    "\n",
    "# Thanks to the procrustes, we can find the optimal solution for W thanks to the SVD(Y_T * X)\n",
    "U, S, V_T = scipy.linalg.svd( (Y_T@X) )\n",
    "\n",
    "# We have W* = U*V_T where U*S*V_T = SVD(Y_T * X)\n",
    "W = U @ V_T\n",
    "\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French words' nearest neighbors in English\n",
      "chat ['squirrel' 'poodle' 'feline' 'hamster' 'rabbit']\n",
      "rideau ['windscreen' 'rideau' 'balcony' 'foyer' 'curtains']\n",
      "pomme ['spinach' 'papaya' 'potatoes' 'potato' 'avocado']\n",
      "stylo ['syringe' 'headset' 'ink' 'flashlight' 'typewriter']\n",
      "\n",
      "\n",
      "English words' nearest neighbors in French\n",
      "dog ['chicken' 'chienne' 'chiens' 'hound' 'chien']\n",
      "computer ['informatiques' 'computing' 'ordinateur' 'ordinateurs' 'informatique']\n",
      "bread ['saucisses' 'beurre' 'desserts' 'confiture' 'pains']\n",
      "flower ['leaves' 'p√©tales' 'fleur' 'flowers' 'fleurs']\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "# We need to calculate the transposed of W for our function\n",
    "W_T = np.transpose(W)\n",
    "\n",
    "def translated_nn( word, K, language = \"French\"):\n",
    "    if language == \"French\":\n",
    "        key_list = []\n",
    "        key_score = []\n",
    "        # Creating an array with the cosine score for each word\n",
    "        for key, value in w2v_en.word2vec.items():\n",
    "            key_list.append(key)\n",
    "            new_coord = W_T @ w2v_fr.word2vec[word]\n",
    "            cosine_similarity = ( np.dot(new_coord, w2v_en.word2vec[key]) ) / ( np.linalg.norm( new_coord ) * np.linalg.norm( w2v_en.word2vec[key] )) \n",
    "            key_score.append( cosine_similarity )\n",
    "        # Sorting the words\n",
    "        key_list = np.array(key_list)\n",
    "        key_score = np.array(key_score)\n",
    "        sorted_score = np.argsort( key_score )\n",
    "        most_similar_list = key_list[sorted_score[(-K-1):-1]]\n",
    "        return most_similar_list\n",
    "    elif language == \"English\":\n",
    "        key_list = []\n",
    "        key_score = []\n",
    "        # Creating an array with the cosine score for each word\n",
    "        for key, value in w2v_fr.word2vec.items():\n",
    "            key_list.append(key)\n",
    "            new_coord = W @ w2v_en.word2vec[word]\n",
    "            cosine_similarity = ( np.dot(new_coord, w2v_fr.word2vec[key]) ) / ( np.linalg.norm( new_coord ) * np.linalg.norm( w2v_fr.word2vec[key] )) \n",
    "            key_score.append( cosine_similarity )\n",
    "        # Sorting the words\n",
    "        key_list = np.array(key_list)\n",
    "        key_score = np.array(key_score)\n",
    "        sorted_score = np.argsort( key_score )\n",
    "        most_similar_list = key_list[sorted_score[(-K-1):-1]]\n",
    "        return most_similar_list\n",
    "    else:\n",
    "        print(\"Write 'French' or 'English' \")\n",
    "        \n",
    "french_words = ['chat', 'rideau', 'pomme', 'stylo']\n",
    "english_words = ['dog', 'computer', 'bread', 'flower']\n",
    "\n",
    "print(\"French words' nearest neighbors in English\")\n",
    "for item in french_words:\n",
    "    print(item, translated_nn( item, K=5, language = \"French\"))\n",
    "\n",
    "print('\\n')\n",
    "                                                                               \n",
    "print(\"English words' nearest neighbors in French\")\n",
    "for item in english_words:\n",
    "    print(item, translated_nn( item, K=5, language = \"English\"))                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "PATH_TO_DATA_3 = \"data/SST/\"\n",
    "\n",
    "sentences_train = []\n",
    "results_train = []\n",
    "\n",
    "with io.open(os.path.join(PATH_TO_DATA_3, 'stsa.fine.train'), encoding='utf-8', newline='\\n') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        sentences_train.append(line[1:-1])\n",
    "        results_train.append(int(line[:1]))\n",
    "        \n",
    "sentences_test = []\n",
    "\n",
    "with io.open(os.path.join(PATH_TO_DATA_3, 'stsa.fine.test.X'), encoding='utf-8', newline='\\n') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        sentences_test.append(line[:-1])\n",
    "        \n",
    "sentences_dev = []\n",
    "results_dev = []\n",
    "\n",
    "with io.open(os.path.join(PATH_TO_DATA_3, 'stsa.fine.dev'), encoding='utf-8', newline='\\n') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        sentences_dev.append(line[1:-1])\n",
    "        results_dev.append(int(line[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# TYPE CODE HERE \n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "encoded_train = s2v.encode(sentences_train)\n",
    "encoded_test = s2v.encode(sentences_test)\n",
    "encoded_dev = s2v.encode(sentences_dev)\n",
    "\n",
    "# Let us also encode with the idf\n",
    "idf_train = s2v.build_idf(sentences_train)\n",
    "idf_test = s2v.build_idf(sentences_test)\n",
    "idf_dev = s2v.build_idf(sentences_dev)\n",
    "\n",
    "encoded_train_idf = s2v.encode(sentences_train, idf_train)\n",
    "encoded_test_idf = s2v.encode(sentences_test, idf_test)\n",
    "encoded_dev_idf = s2v.encode(sentences_dev, idf_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned hyperparameters :(best parameters)  {'C': 1.0, 'fit_intercept': False}\n",
      "Train accuracy with no idf: 0.4993561980568887\n",
      "Dev accuracy with no idf: 0.43272727272727274\n",
      "Train accuracy with idf: 0.497600374575676\n",
      "Dev accuracy with idf: 0.4090909090909091\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params_logreg = {'fit_intercept':(True, False), 'C':np.logspace(-3,3,7)}\n",
    "\n",
    "logreg = LogisticRegression(random_state=8, solver='lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "logreg_cv = GridSearchCV(logreg, params_logreg, cv=5)\n",
    "logreg_cv.fit(encoded_train, results_train)\n",
    "\n",
    "print(\"Tuned hyperparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"Train accuracy with no idf:\",logreg_cv.score(encoded_train,results_train))\n",
    "print(\"Dev accuracy with no idf:\",logreg_cv.score(encoded_dev,results_dev))\n",
    "\n",
    "logreg_idf= LogisticRegression(random_state=8, C=1, solver='lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "logreg_idf.fit(encoded_train_idf, results_train)\n",
    "print(\"Train accuracy with idf:\",logreg_idf.score(encoded_train_idf,results_train))\n",
    "print(\"Dev accuracy with idf:\",logreg_idf.score(encoded_dev_idf,results_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "results_test_1 = logreg_cv.predict(encoded_test)\n",
    "\n",
    "file = open(\"logreg_bov_y_test_sst.txt\",\"w\") \n",
    "\n",
    "for i in results_test_1:\n",
    "    file.write(str(i)) \n",
    "    file.write(\"\\n\")\n",
    "\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned hyperparameters : {'kernel': 'rbf', 'degree': 3, 'C': 100}\n",
      "Accuracy : 0.4892894767646026\n",
      "Test score : 0.43363636363636365\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params_svc = { 'C':np.logspace(-3,3,7),'degree': range(1,6), 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "svc=SVC(random_state = 8, gamma='auto')\n",
    "svc_cv = RandomizedSearchCV(svc, params_svc, cv=5)\n",
    "svc_cv.fit(encoded_train, results_train)\n",
    "\n",
    "print(\"Tuned hyperparameters :(best parameters) \",svc_cv.best_params_)\n",
    "print(\"Accuracy :\",svc_cv.best_score_)\n",
    "\n",
    "#After fitting the randomized search, those were the best parameters\n",
    "svc_2 = SVC(gamma = 'auto', random_state=8, C = 100 , degree = 3 , kernel = 'rbf' )\n",
    "svc_2.fit(encoded_train, results_train)\n",
    "\n",
    "print(\"Test score :\", svc_2.score(encoded_dev, results_dev))\n",
    "\n",
    "# Creating a file for the results of the test set\n",
    "results_test_2 = svc_2.predict(encoded_test)\n",
    "\n",
    "file = open(\"svc_bov_y_test_sst.txt\",\"w\") \n",
    "\n",
    "for i in results_test_2:\n",
    "    file.write(str(i)) \n",
    "    file.write(\"\\n\")\n",
    "\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "\n",
    "# TYPE CODE HERE\n",
    "PATH_TO_DATA_3 = \"data/SST/\"\n",
    "\n",
    "sentences_train = []\n",
    "results_train = []\n",
    "\n",
    "with io.open(os.path.join(PATH_TO_DATA_3, 'stsa.fine.train'), encoding='utf-8', newline='\\n') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        sentences_train.append(line[1:-1])\n",
    "        results_train.append(int(line[:1]))\n",
    "        \n",
    "sentences_test = []\n",
    "\n",
    "with io.open(os.path.join(PATH_TO_DATA_3, 'stsa.fine.test.X'), encoding='utf-8', newline='\\n') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        sentences_test.append(line[:-1])\n",
    "        \n",
    "sentences_dev = []\n",
    "results_dev = []\n",
    "\n",
    "with io.open(os.path.join(PATH_TO_DATA_3, 'stsa.fine.dev'), encoding='utf-8', newline='\\n') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        sentences_dev.append(line[1:-1])\n",
    "        results_dev.append(int(line[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "# Finding the vocab size : We split each sentence into words for the three sets of text\n",
    "all_text = (sentences_dev+ sentences_test + sentences_train)\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "a = []\n",
    "for line in all_text:\n",
    "        a.append(text_to_word_sequence(line))\n",
    "# We then flatten it to get a final list of all words, and use \"set\" to count the unique words\n",
    "flat_list = [item for sublist in a for item in sublist]\n",
    "vocab_size = len(set(flat_list))\n",
    "\n",
    "n = vocab_size\n",
    "\n",
    "# We then transform the text into integers (word indexes) thanks to the one_hot preprocessing function\n",
    "one_hot_train = []\n",
    "one_hot_test = []\n",
    "one_hot_dev = []\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "for sent in sentences_train:\n",
    "    one_hot_train.append( one_hot(sent, n, lower=True, split=' '))\n",
    "    \n",
    "for sent in sentences_test:\n",
    "    one_hot_test.append( one_hot(sent, n, lower=True, split=' '))\n",
    "    \n",
    "for sent in sentences_dev:\n",
    "    one_hot_dev.append( one_hot(sent, n, lower=True, split=' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# To get maxlen, we ran the padding once and then used tha maximum shape[0] of our three sets\n",
    "\n",
    "pad_train = pad_sequences(one_hot_train, maxlen = 52)\n",
    "pad_test = pad_sequences(one_hot_test, maxlen = 52)\n",
    "pad_dev = pad_sequences(one_hot_dev, maxlen = 52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juann\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(256, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "\n",
    "\n",
    "embed_dim  = 52 # word embedding dimension\n",
    "nhid       = 256  # number of hidden units in the LSTM\n",
    "vocab_size = vocab_size #100000  # size of the vocabulary found previously\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 52)          927524    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               316416    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 1,245,225\n",
      "Trainable params: 1,245,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # We selected this classifier since it it a multiclass probelm\n",
    "optimizer        =  'adam' # We used the Adam optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juann\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8543 samples, validate on 1100 samples\n",
      "Epoch 1/2\n",
      "8543/8543 [==============================] - 196s 23ms/step - loss: 1.5691 - acc: 0.2871 - val_loss: 1.5206 - val_acc: 0.3373\n",
      "Epoch 2/2\n",
      "8543/8543 [==============================] - 180s 21ms/step - loss: 1.3750 - acc: 0.3926 - val_loss: 1.4076 - val_acc: 0.3836\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 32\n",
    "#We take 2 epochs to avoid overfitting\n",
    "n_epochs = 2\n",
    "\n",
    "# We used the dev set as validation set\n",
    "x_train = pad_train\n",
    "y_train_1 = results_train\n",
    "x_val = pad_dev\n",
    "y_val_1 = results_dev\n",
    "\n",
    "# Transforming our results into categorical data\n",
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(y_train_1, 5)\n",
    "y_val = np_utils.to_categorical(y_val_1, 5)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_val, y_val))\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model train vs validation acc')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "x_test = pad_test\n",
    "\n",
    "results_test_3 = model.predict(x_test)\n",
    "argmax_3 = np.argmax(results_test_3, axis = 1)\n",
    "\n",
    "file = open(\"logreg_lstm_y_test_sst.txt\",\"w\") \n",
    "\n",
    "for i in argmax_3:\n",
    "    file.write(str(i)) \n",
    "    file.write(\"\\n\")\n",
    "\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "Train on 7688 samples, validate on 855 samples\n",
      "Epoch 1/1\n",
      "7688/7688 [==============================] - 29s 4ms/step - loss: 1.4633 - acc: 0.3554 - val_loss: 1.3523 - val_acc: 0.4000\n",
      "1100/1100 [==============================] - 1s 1ms/step\n",
      "Validation accuracy: 0.4118181819265539\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "# I decided to use another encoder. I used the Tokenizer function to preprocess the text.\n",
    "# Each input line would therefore have a size (,vocab_size) instead of (,52).\n",
    "# This higher dimension might lead to enhanced prediction.\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "# fit the tokenizer on the train text\n",
    "t.fit_on_texts(sentences_train+sentences_dev+sentences_test)\n",
    "\n",
    "# integer encode documents\n",
    "token_train = t.texts_to_matrix(sentences_train, mode='count')\n",
    "token_test = t.texts_to_matrix(sentences_test, mode='count')\n",
    "token_dev = t.texts_to_matrix(sentences_dev, mode='count')\n",
    "\n",
    "# We will use dev set as validation set\n",
    "x_train = token_train\n",
    "y_train_1 = results_train\n",
    "x_val = token_dev\n",
    "y_val_1 = results_dev\n",
    "\n",
    "# Transforming reluts into categorical data\n",
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(y_train_1, 5)\n",
    "y_val = np_utils.to_categorical(y_val_1, 5)\n",
    "\n",
    "# I used a classical dense NN, which improved the previous results. \n",
    "# Any attempt to perform a Conv1D or an LSTM lead to a very high computational time, and no extra performance.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras import regularizers\n",
    "\n",
    "model_dnn = Sequential()\n",
    "model_dnn.add(Dense(64, input_shape=(x_train.shape[1],), activation='relu'))\n",
    "model_dnn.add(Dropout(0.2))\n",
    "model_dnn.add(Dense(128, activation='relu'))\n",
    "model_dnn.add(Dropout(0.2))\n",
    "\n",
    "model_dnn.add(Dense(5))\n",
    "model_dnn.add(Activation('softmax'))\n",
    "\n",
    "model_dnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_dnn.metrics_names)\n",
    "\n",
    "# To avoid overfitting, I had to use only one epoch, since the model converges fast\n",
    "batch_size = 16\n",
    "epochs = 1\n",
    "\n",
    "history = model_dnn.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "score = model_dnn.evaluate(x_val, y_val, batch_size=batch_size, verbose=1)\n",
    "print('Validation accuracy:', score[1])\n",
    "\n",
    "\n",
    "# Let us now predict the results and store those\n",
    "x_test = token_test\n",
    "\n",
    "results_test_4 = model_dnn.predict(x_test)\n",
    "argmax_4 = np.argmax(results_test_4, axis = 1)\n",
    "\n",
    "file = open(\"tokenized_denseNN_y_test_sst.txt\",\"w\") \n",
    "\n",
    "for i in argmax_4:\n",
    "    file.write(str(i)) \n",
    "    file.write(\"\\n\")\n",
    "\n",
    "file.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
